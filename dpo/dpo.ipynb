{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "124a869a",
   "metadata": {},
   "source": [
    "### Step 1: Install necesscary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b82f8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib torch numpy transformers datasets tiktoken wandb tqdm --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2d9de0",
   "metadata": {},
   "source": [
    "### Step 2: Package imports and configuration\n",
    "\n",
    "- Tokeniser appears to not have !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "876dd92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ngxua/Documents/ComputerScience_Projects/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: mps\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\")) \n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import pickle\n",
    "from model import GPT, GPTConfig\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import GPT2TokenizerFast, GPT2Tokenizer\n",
    "\n",
    "# Hyperparameters for training and generation\n",
    "beta = 0.5\n",
    "if torch.backends.mps.is_available():\n",
    "    # For Macbook\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    # For Windows\n",
    "    device = 'cuda'  \n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Base learning rate - For controlling how much the weights are updated. Smaller is better to prevent overshoot but will take longer\n",
    "base_lr = 1e-4\n",
    "# Number of times model will see the training dataset. Not too many to prevent overfitting\n",
    "epochs = 8\n",
    "# Number of samples before updating the model's weights\n",
    "batch_size = 64\n",
    "# Max number of tokens in each input sequence, i.e. the length the model can see at once\n",
    "max_length = 64\n",
    "num_samples = 1\n",
    "# Max number of tokens to generate in answer\n",
    "max_new_tokens = 200\n",
    "# Controls randomness in generation of tokens. Lower values (close to 0) make output more deterministic\n",
    "temperature = 0.8\n",
    "# Samples from top k most likely tokens to generate next\n",
    "top_k = 200\n",
    "\n",
    "# Loads a pickled dictionary, which is a saved dict\n",
    "with open(\"../sft/meta.pkl\", \"rb\") as f:\n",
    "    meta = pickle.load(f)\n",
    "stoi, itos = meta[\"stoi\"], meta[\"itos\"]\n",
    "def encode(s): \n",
    "    return [stoi[c] for c in s]\n",
    "def decode(l): \n",
    "    return ''.join([itos[i] for i in l])\n",
    "\n",
    "# Verify device used\n",
    "print(f\"Using: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7d35e6",
   "metadata": {},
   "source": [
    "### Step 3: Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d03655c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Used for scoring various sequences of tokens generated by the GPT to decide which sequence should be returned\n",
    "\"\"\"\n",
    "def compute_logprob(input_ids):\n",
    "    # All tokens except for the last token is used as input\n",
    "    inputs = input_ids[:, :-1]\n",
    "    # All tokens except for the first, i.e. the next target token that should be predicted\n",
    "    targets = input_ids[:, 1:]\n",
    "    # Runs the model to get the logits (unnormalized scores) for each possible next token at each position\n",
    "    logits, _ = gpt(inputs, full_seq=True)\n",
    "\n",
    "    # Reshape both the logits and targets to the correct shape, i.e. (batch_size * seq_length)\n",
    "    B, T, V = logits.size()\n",
    "    logits_flat = logits.reshape(-1, V)\n",
    "    targets_flat = targets.reshape(-1)\n",
    "    # Calculates the cross-entropy loss (negative log-probability) for each token\n",
    "    loss = F.cross_entropy(logits_flat, targets_flat, ignore_index=0, reduction='none')\n",
    "    loss = loss.reshape(B, T)\n",
    "    # Creates a mask to ignore padding tokens\n",
    "    attention_mask = (targets != 0).float()\n",
    "    loss = (loss * attention_mask).sum(dim=1) / attention_mask.sum(dim=1)\n",
    "    # Returns the negative loss, which is the average log-probability per sequence (higher is better, since log-probs are negative)\n",
    "    return -loss \n",
    "\n",
    "\"\"\"\n",
    "Ensures that every sequence is exactly max_length tokens long, either by truncating longer sequences or padding shorter ones with zeros (the padding token)\n",
    "\n",
    "Either keep the last max_length tokens if length exceeds.\n",
    "Or pads with 0s to reach the end\n",
    "\"\"\"\n",
    "def pad_or_truncate(seq, max_length):\n",
    "    return seq[-max_length:] if len(seq) > max_length else seq + [0] * (max_length - len(seq))\n",
    "\n",
    "\"\"\"\n",
    "Prepares batches of positive and negative examples for training or evaluation, yielding them as tensors ready for the model\n",
    "\"\"\"\n",
    "def get_batches(lines, batch_size):\n",
    "    # Randomise the order of data for each epoch\n",
    "    random.shuffle(lines)\n",
    "    #for l in lines:\n",
    "    #    print(l[1])\n",
    "    for i in range(0, len(lines), batch_size):\n",
    "        batch = lines[i:i+batch_size]\n",
    "        # Skips the last batch if the size is too small to make sure all batches have the same size\n",
    "        if len(batch) < batch_size:\n",
    "            continue\n",
    "        # Encode the negative and positive strings and pad/truncate to maxLenght\n",
    "        neg_inputs = [pad_or_truncate(encode(p['negative'] + '\\n\\n\\n\\n'), max_length) for p in batch]\n",
    "        pos_inputs = [pad_or_truncate(encode(p['positive'] + '\\n\\n\\n\\n'), max_length) for p in batch]\n",
    "        # Converts the lists of token IDs into PyTorch tensors on the correct device.\n",
    "        neg_tensor = torch.tensor(neg_inputs, dtype=torch.long, device=device)\n",
    "        pos_tensor = torch.tensor(pos_inputs, dtype=torch.long, device=device)\n",
    "        # Yields a tuple (neg_tensor, pos_tensor) for each batch, as tensors are used for model input\n",
    "        yield neg_tensor, pos_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9d9eba",
   "metadata": {},
   "source": [
    "### Step 4: Load the pretrained NanoGPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ceae772a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(74, 348)\n",
       "    (wpe): Embedding(256, 348)\n",
       "    (drop): Dropout(p=0.2, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=348, out_features=1044, bias=False)\n",
       "          (c_proj): Linear(in_features=348, out_features=348, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.2, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=348, out_features=1392, bias=False)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=1392, out_features=348, bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=348, out_features=74, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pre-trained model, i.e. the checkpoint, which has all the model weights\n",
    "ckpt = torch.load(\"../sft/gpt.pt\", map_location=device)\n",
    "# Recreate the model configuration with the saved details\n",
    "gptconf = GPTConfig(**ckpt['model_args'])\n",
    "# Create a new instance of the model using the pre-trained configs\n",
    "gpt = GPT(gptconf)\n",
    "\n",
    "# Clean up any keys that might have unwanted prefixes while saving\n",
    "state_dict = ckpt['model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k in list(state_dict.keys()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "\n",
    "# Load the model weights from the checkpoint to the new model instance\n",
    "gpt.load_state_dict(state_dict)\n",
    "# Set it to training mode after ensuring it runs on GPU if it exists\n",
    "gpt.to(device).train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feafc5a",
   "metadata": {},
   "source": [
    "### Step 5: Load Data (**students are required to complete this part!**)\n",
    "\n",
    "- Currently I generated 5000 simple arithmetic like the ones provided, and 5000 more 2 step arithmetic like 2*(5+3)=?\n",
    "- Would need to generate more until at least 100k in 1:1 ratio\n",
    "- Then maybe 50k more for ones that uses brackets like (x+5)*2=30,x=?. Need about 50k to maintain the ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7edf3d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Training Datapoints:  100000\n",
      "Number of Cleaned Datapoints: 100000\n"
     ]
    }
   ],
   "source": [
    "# Load my training dataset\n",
    "with open(\"./pos_neg_pairs.json\") as file:\n",
    "    lines = json.load(file)\n",
    "\n",
    "# Filter out characters like ! that have not been encoded by the trained model\n",
    "num_clean_datapoints = 0\n",
    "for line in lines:\n",
    "    positive_datapoint = line[\"positive\"]\n",
    "    negative_datapoint = line[\"negative\"]\n",
    "\n",
    "    # Clean both datapoints\n",
    "    cleaned_positive_datapoint = ''.join(char for char in positive_datapoint if char in stoi)\n",
    "    cleaned_negative_datapoint = ''.join(char for char in negative_datapoint if char in stoi)\n",
    "\n",
    "    # Check if cleaned\n",
    "    if cleaned_positive_datapoint != positive_datapoint or negative_datapoint != cleaned_negative_datapoint:\n",
    "        num_clean_datapoints += 1\n",
    "    \n",
    "    # Update with cleaned\n",
    "    line[\"positive\"] = cleaned_positive_datapoint\n",
    "    line[\"negative\"] = cleaned_negative_datapoint\n",
    "\n",
    "print(f\"Number of Training Datapoints:  {len(lines)}\")\n",
    "print(f\"Number of Cleaned Datapoints: {num_clean_datapoints}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e5f81f",
   "metadata": {},
   "source": [
    "### Step 6: Build the optimizer and scheduler (**students are required to complete this part!**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df0c400f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommended by prof to use the AdamW optimizer\n",
    "## Apply a weight decay which acts as a regulariser to reduce overfitting\n",
    "adamw_optimizer = torch.optim.AdamW(gpt.parameters(), lr=base_lr, weight_decay=0.1)\n",
    "\n",
    "## Scheduler helps adjust the learning rate of the optimiser while training. Idea is to have learning rate be high and decrease slowly\n",
    "## Cosine is pretty good cos the graph is high then low\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(adamw_optimizer, T_max=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b66199",
   "metadata": {},
   "source": [
    "### Step 7: Begin training (**students are required to complete this part!**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d4ebeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Step 1562 Loss 0.0768: : 1562it [08:18,  3.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Step 1562 Loss 0.0591: : 1562it [08:34,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Step 1562 Loss 0.0382: : 1562it [08:30,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Step 1562 Loss 0.0352: : 1562it [08:45,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 Step 1562 Loss 0.0320: : 1562it [08:29,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 Step 1562 Loss 0.0305: : 1562it [08:27,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 Step 1562 Loss 0.0313: : 1562it [08:30,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 Step 1562 Loss 0.0291: : 1562it [08:28,  3.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Lines refer to the number of lines of training data we have so we know how much steps/batches is req to go thru dataset once\n",
    "total_steps = len(lines) // batch_size\n",
    "\n",
    "# Loop over the dataset for different epochs\n",
    "for epoch in range(epochs):\n",
    "    # Retrieve the batches of positive and negative tensors from the helper function we saw earlier\n",
    "    pbar = tqdm(get_batches(lines, batch_size))\n",
    "    for step, (neg_tensor,pos_tensor) in enumerate(pbar):\n",
    "        # 1. Zero the accumulated gradients from previous batch of training or epoch of training. Do once at the start of every batch\n",
    "        adamw_optimizer.zero_grad()\n",
    "        # 2. Calculate the actual loss using the loss function in helper. We use DPO\n",
    "        ## First calculate the log probs using the helper\n",
    "        positive_log_prob = compute_logprob(pos_tensor)\n",
    "        negative_log_prob = compute_logprob(neg_tensor)\n",
    "        ## Assign higher weights to positive completions in this case, so the model knows what to do.\n",
    "        dpo_loss = -F.logsigmoid((positive_log_prob - negative_log_prob) / beta).mean() - positive_log_prob.mean() * 0.1\n",
    "        # 3. Backward Propagation, i.e. compute gradient of loss w.r.t model params\n",
    "        dpo_loss.backward()\n",
    "        # 4. Use optimiser to update the model params using new gradients to reduce dpo_loss\n",
    "        adamw_optimizer.step()\n",
    "        # 5. Update the progress bar so we can see how the training is going\n",
    "        pbar.set_description(f\"Epoch {epoch+1} Step {step+1} Loss {dpo_loss.item():.4f}\")\n",
    "        # 6. Ensures the scheduler decays the learning rate after every epoch\n",
    "        scheduler.step()\n",
    "\n",
    "    # Specify where to save our trained model\n",
    "    ckpt_path = f\"./dpo.pt\"\n",
    "    # Saves the model's weights and configuration after each epoch, so you can resume training or use the model later\n",
    "    torch.save({\n",
    "        \"model_state_dict\": gpt.state_dict(),\n",
    "        \"model_args\": ckpt['model_args'],\n",
    "    }, ckpt_path)\n",
    "    print(f\"Saved checkpoint to {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88403d48",
   "metadata": {},
   "source": [
    "- The loss never stopped decreasing, so perhaps we can increase the number of epochs, since it have not overfitted on the train set yet. Even though we are using 10k only for now.\n",
    "- can increase epoch since the dpo loss did not plateau or increase\n",
    "- can consider increase batch size to 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b7f2ab",
   "metadata": {},
   "source": [
    "### Step 8: Begin testing (**students are required to complete this part!**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09027262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the fine-tuned model that we trained earlier above\n",
    "ckpt_path = \"./dpo.pt\"\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "\n",
    "# Load the saved model args\n",
    "gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "gpt = GPT(gptconf).to(device)\n",
    "\n",
    "# Load the model saved weights\n",
    "state_dict = checkpoint['model_state_dict']\n",
    "\n",
    "# Clean the data keys in dict again. Remember the keys might get affected whenever we save a checkpoint\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "gpt.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21ca1ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 17+19=?\n",
      "Output: 17+19=? The answer is 20 because 17+19 equals 20.\n",
      "\n",
      "Prompt: 3*17=?\n",
      "Output: 3*17=? The answer is 51 because 3 times 17 equals 51.\n",
      "\n",
      "Prompt: 72/4=?\n",
      "Output: 72/4=? The answer is 11 because 72/4 equals 11.\n",
      "\n",
      "Prompt: 72-x=34,x=?\n",
      "Output: 72-x=34,x=? The answer is 82 because 34-0 equals to 822.\n",
      "\n",
      "Prompt: x*11=44,x=?\n",
      "Output: x*11=44,x=? The answer is 4 because 44/11 equals to 4.\n",
      "\n",
      "Prompt: 3*17=?\n",
      "Output: 3*17=? The answer is 51 because 3 times 17 equals 51.\n",
      "\n",
      "Prompt: 72/4=?\n",
      "Output: 72/4=? The answer is 12 because 72/4 equals 12.\n",
      "\n",
      "Prompt: 72-x=34,x=?\n",
      "Output: 72-x=34,x=? The answer is 270 because 34+92 equals to 270.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test - Set the Model to Eval mode for deterministic testing behaviur\n",
    "gpt.eval()\n",
    "# Load the test set\n",
    "# test_set = [\"1*1=?\"]\n",
    "test_set = [\"17+19=?\", \"3*17=?\", \"72/4=?\", \"72-x=34,x=?\", \"x*11=44,x=?\", \"3*17=?\", \"72/4=?\", \"72-x=34,x=?\"]\n",
    "\n",
    "# Disable gradient computation since we are not training the model anymore\n",
    "with torch.no_grad():\n",
    "    for prompt in test_set: \n",
    "        # Encode the string and convert it to a tensor to be used as input\n",
    "        test_tensor = torch.tensor(encode(prompt), dtype=torch.long, device=device).unsqueeze(0)\n",
    "        # Use the test tensors to generate an answer\n",
    "        answer, _ = gpt.generate(test_tensor, max_new_tokens=max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "        # Decode ans back for human understanding\n",
    "        output = decode(answer[0].tolist())\n",
    "        print(f\"Prompt: {prompt}\\nOutput: {output}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51ef8bd",
   "metadata": {},
   "source": [
    "- I increased the dataset to 100k and batchsize to 128, epochs to 10 in Google Colab but the model still cant answer math questions well.\n",
    "- 10 epochs started to overfit actually\n",
    "- Tokeniser provided appears to not treat double digit ints as ints but separate numbre\n",
    "- Retrain the model with a better tokeniser tmr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
