{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "124a869a",
   "metadata": {},
   "source": [
    "### Step 1: Install necesscary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b82f8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib torch numpy transformers datasets tiktoken wandb tqdm --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2d9de0",
   "metadata": {},
   "source": [
    "### Step 2: Package imports and configuration\n",
    "\n",
    "- Tokeniser appears to not have !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "876dd92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\")) \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import pickle\n",
    "from model import GPT, GPTConfig\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configuration\n",
    "## Hyperparameters for training and generation\n",
    "beta = 0.5\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # Choose GPU if it exists\n",
    "base_lr = 1e-4\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "max_length =64\n",
    "num_samples = 1\n",
    "max_new_tokens = 200\n",
    "temperature = 0.8\n",
    "top_k = 200\n",
    "\n",
    "# Tokenizer - Loads a pickled dictionary, which is a saved dict\n",
    "## Contains stoi and iots\n",
    "with open(\"../sft/meta.pkl\", \"rb\") as f:\n",
    "    meta = pickle.load(f)\n",
    "stoi, itos = meta[\"stoi\"], meta[\"itos\"]\n",
    "# Converts a string into a list of token IDs\n",
    "def encode(s): \n",
    "    # TODO Drop unknown chars like ! for now\n",
    "    return [stoi[c] for c in s if c in stoi]\n",
    "# Convert a list of token IDs back into a string\n",
    "def decode(l): \n",
    "    return ''.join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7d35e6",
   "metadata": {},
   "source": [
    "### Step 3: Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d03655c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Used for scoring various sequences of tokens generated by the GPT to decide which sequence should be returned\n",
    "\"\"\"\n",
    "def compute_logprob(input_ids):\n",
    "    # All tokens except for the last token is used as input\n",
    "    inputs = input_ids[:, :-1]\n",
    "    # All tokens except for the first, i.e. the next target token that should be predicted\n",
    "    targets = input_ids[:, 1:]\n",
    "    # Runs the model to get the logits (unnormalized scores) for each possible next token at each position\n",
    "    logits, _ = gpt(inputs, full_seq=True)\n",
    "\n",
    "    # Reshape both the logits and targets to the correct shape, i.e. (batch_size * seq_length)\n",
    "    B, T, V = logits.size()\n",
    "    logits_flat = logits.reshape(-1, V)\n",
    "    targets_flat = targets.reshape(-1)\n",
    "    # Calculates the cross-entropy loss (negative log-probability) for each token\n",
    "    loss = F.cross_entropy(logits_flat, targets_flat, ignore_index=0, reduction='none')\n",
    "    loss = loss.reshape(B, T)\n",
    "    # Creates a mask to ignore padding tokens\n",
    "    attention_mask = (targets != 0).float()\n",
    "    loss = (loss * attention_mask).sum(dim=1) / attention_mask.sum(dim=1)\n",
    "    # Returns the negative loss, which is the average log-probability per sequence (higher is better, since log-probs are negative)\n",
    "    return -loss \n",
    "\n",
    "\"\"\"\n",
    "Ensures that every sequence is exactly max_length tokens long, either by truncating longer sequences or padding shorter ones with zeros (the padding token)\n",
    "\n",
    "Either keep the last max_length tokens if length exceeds.\n",
    "Or pads with 0s to reach the end\n",
    "\"\"\"\n",
    "def pad_or_truncate(seq, max_length):\n",
    "    return seq[-max_length:] if len(seq) > max_length else seq + [0] * (max_length - len(seq))\n",
    "\n",
    "\"\"\"\n",
    "Prepares batches of positive and negative examples for training or evaluation, yielding them as tensors ready for the model\n",
    "\"\"\"\n",
    "def get_batches(lines, batch_size):\n",
    "    # Randomise the order of data for each epoch\n",
    "    random.shuffle(lines)\n",
    "    #for l in lines:\n",
    "    #    print(l[1])\n",
    "    for i in range(0, len(lines), batch_size):\n",
    "        batch = lines[i:i+batch_size]\n",
    "        # Skips the last batch if the size is too small to make sure all batches have the same size\n",
    "        if len(batch) < batch_size:\n",
    "            continue\n",
    "        # Encode the negative and positive strings and pad/truncate to maxLenght\n",
    "        neg_inputs = [pad_or_truncate(encode(p['negative'] + '\\n\\n\\n\\n'), max_length) for p in batch]\n",
    "        pos_inputs = [pad_or_truncate(encode(p['positive'] + '\\n\\n\\n\\n'), max_length) for p in batch]\n",
    "        # Converts the lists of token IDs into PyTorch tensors on the correct device.\n",
    "        neg_tensor = torch.tensor(neg_inputs, dtype=torch.long, device=device)\n",
    "        pos_tensor = torch.tensor(pos_inputs, dtype=torch.long, device=device)\n",
    "        # Yields a tuple (neg_tensor, pos_tensor) for each batch, as tensors are used for model input\n",
    "        yield neg_tensor, pos_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9d9eba",
   "metadata": {},
   "source": [
    "### Step 4: Load the pretrained NanoGPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ceae772a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(74, 348)\n",
       "    (wpe): Embedding(256, 348)\n",
       "    (drop): Dropout(p=0.2, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=348, out_features=1044, bias=False)\n",
       "          (c_proj): Linear(in_features=348, out_features=348, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.2, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=348, out_features=1392, bias=False)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=1392, out_features=348, bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=348, out_features=74, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pre-trained model, i.e. the checkpoint, which has all the model weights\n",
    "ckpt = torch.load(\"../sft/gpt.pt\", map_location=device)\n",
    "# Recreate the model configuration with the saved details\n",
    "gptconf = GPTConfig(**ckpt['model_args'])\n",
    "# Create a new instance of the model using the pre-trained configs\n",
    "gpt = GPT(gptconf)\n",
    "\n",
    "# Clean up any keys that might have unwanted prefixes while saving\n",
    "state_dict = ckpt['model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k in list(state_dict.keys()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "\n",
    "# Load the model weights from the checkpoint to the new model instance\n",
    "gpt.load_state_dict(state_dict)\n",
    "# Set it to training mode after ensuring it runs on GPU if it exists\n",
    "gpt.to(device).train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feafc5a",
   "metadata": {},
   "source": [
    "### Step 5: Load Data (**students are required to complete this part!**)\n",
    "\n",
    "- Currently I generated 5000 simple arithmetic like the ones provided, and 5000 more 2 step arithmetic like 2*(5+3)=?\n",
    "- Would need to generate more until at least 100k in 1:1 ratio\n",
    "- Then maybe 50k more for ones that uses brackets like (x+5)*2=30,x=?. Need about 50k to maintain the ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7edf3d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lines' size:  10024\n"
     ]
    }
   ],
   "source": [
    "# Load the positive and negative pairs\n",
    "with open(\"./pos_neg_pairs.json\") as file:\n",
    "    lines = json.load(file)\n",
    "print(f\"lines' size:  {len(lines)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e5f81f",
   "metadata": {},
   "source": [
    "### Step 6: Build the optimizer and scheduler (**students are required to complete this part!**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df0c400f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recommend to use the AdamW optimizer\n",
    "adamw_optimizer = torch.optim.AdamW(gpt.parameters(), lr=base_lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(adamw_optimizer, T_max=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b66199",
   "metadata": {},
   "source": [
    "### Step 7: Begin training (**students are required to complete this part!**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d4ebeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Step 156 Loss 0.1232: : 156it [03:11,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Step 156 Loss 0.0952: : 156it [03:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Step 156 Loss 0.0866: : 156it [02:50,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Step 156 Loss 0.0754: : 156it [03:02,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 Step 156 Loss 0.0695: : 156it [02:59,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Lines refer to the number of lines of training data we have so we know how much steps/batches is req to go thru dataset once\n",
    "total_steps = len(lines) // batch_size\n",
    "\n",
    "# Loop over the dataset for different epochs\n",
    "for epoch in range(epochs):\n",
    "    # Retrieve the batches of positive and negative tensors from the helper function we saw earlier\n",
    "    pbar = tqdm(get_batches(lines, batch_size))\n",
    "    for step, (neg_tensor,pos_tensor) in enumerate(pbar):\n",
    "        # 1. Zero the accumulated gradients from previous batch of training or epoch of training. Do once at the start of every batch\n",
    "        adamw_optimizer.zero_grad()\n",
    "        # 2. Calculate the actual loss using the loss function in helper. We use DPO\n",
    "        ## First calculate the log probs using the helper\n",
    "        positive_log_prob = compute_logprob(pos_tensor)\n",
    "        negative_log_prob = compute_logprob(neg_tensor)\n",
    "        ## Assign higher weights to positive completions in this case, so the model knows what to do.\n",
    "        dpo_loss = -F.logsigmoid((positive_log_prob - negative_log_prob) / beta).mean() - positive_log_prob.mean() * 0.1\n",
    "        # 3. Backward Propagation, i.e. compute gradient of loss w.r.t model params\n",
    "        dpo_loss.backward()\n",
    "        # 4. Use optimiser to update the model params using new gradients to reduce dpo_loss\n",
    "        adamw_optimizer.step()\n",
    "        # 5. Update the progress bar so we can see how the training is going\n",
    "        pbar.set_description(f\"Epoch {epoch+1} Step {step+1} Loss {dpo_loss.item():.4f}\")\n",
    "\n",
    "    # Specify where to save our trained model\n",
    "    ckpt_path = f\"./dpo.pt\"\n",
    "    # Saves the model's weights and configuration after each epoch, so you can resume training or use the model later\n",
    "    torch.save({\n",
    "        \"model_state_dict\": gpt.state_dict(),\n",
    "        \"model_args\": ckpt['model_args'],\n",
    "    }, ckpt_path)\n",
    "    print(f\"Saved checkpoint to {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88403d48",
   "metadata": {},
   "source": [
    "- The loss never stopped decreasing, so perhaps we can increase the number of epochs, since it have not overfitted on the train set yet. Even though we are using 10k only for now.\n",
    "- can increase epoch since the dpo loss did not plateau or increase\n",
    "- can consider increase batch size to 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b7f2ab",
   "metadata": {},
   "source": [
    "### Step 8: Begin testing (**students are required to complete this part!**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd295983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['model_state_dict', 'model_args']\n"
     ]
    }
   ],
   "source": [
    "# Check to see where the weights are stored, cfm all keys\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "print(list(checkpoint.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09027262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the fine-tuned model that we trained earlier above\n",
    "ckpt_path = \"../dpo/dpo.pt\"\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "# Load the saved model args\n",
    "gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "# gpt = GPT(gptconf).cuda()\n",
    "gpt = GPT(gptconf).to(device)\n",
    "\n",
    "# TODO Loading the model's save weights from model key first, then fall back to the second key (I think not necessary to try since we save under the 2nd key?)\n",
    "# try:\n",
    "#     state_dict = checkpoint['model']\n",
    "# except:\n",
    "#     state_dict = checkpoint['model_state_dict']\n",
    "state_dict = checkpoint['model_state_dict']\n",
    "\n",
    "# Clean the data keys in dict again. Remember the keys might get affected whenever we save a checkpoint\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "gpt.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21ca1ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 17+19=?\n",
      "Output: 17+19=?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Whherhe ansere anss is yo   be e you  okeak muak 09Dhe any pek Erish? uandewed 2300 *9 = 887 and 91493.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test - Set the Model to Eval mode for deterministic testing behaviur\n",
    "gpt.eval()\n",
    "# Load the test set\n",
    "test_set = [\"17+19=?\"]\n",
    "# test_set = [\"17+19=?\", \"3*17=?\", \"72/4=?\", \"72-x=34,x=?\", \"x*11=44,x=?\", \"3*17=?\", \"72/4=?\", \"72-x=34,x=?\"]\n",
    "\n",
    "# Disable gradient computation since we are not training the model anymore\n",
    "with torch.no_grad():\n",
    "    for prompt in test_set: \n",
    "        # Encode each string into a list of string IDs, which are the numerical representations of the string using the tokeniser\n",
    "        # Allow us to provide as input\n",
    "        prompt_ids = pad_or_truncate(encode(prompt), max_length)\n",
    "        # Convert to tensors so the model can use them. Note that there is only 1 tensor\n",
    "        test_tensor = torch.tensor(prompt_ids, dtype=torch.long, device=device).unsqueeze(0)\n",
    "        # Use the test tensors to generate an answer\n",
    "        y, _ = gpt.generate(test_tensor, max_new_tokens=max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "        # Decode back for human understanding\n",
    "        output = decode(y[0].tolist())\n",
    "        print(f\"Prompt: {prompt}\\nOutput: {output}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
